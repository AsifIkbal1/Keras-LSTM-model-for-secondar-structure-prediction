{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data\nTo show proof of concept only considering sequences shorter than 32 amino acids. Also experiments suggest to apply a sliding overlapping window of size 3 to the input seqeunces. In the sense of  NLP this artificially creates a corpus consisting of much more different words (>7000 different ngrams compared to 20 amino acids). There has been a lot of research in respresenting protein sequences as sentences of  biological words. ","metadata":{"_uuid":"2affe7e67c27ea839c980ceb991ae1e21af03d63"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv('../input/2018-06-06-ss.cleaned.csv')\ndf.len.hist(bins=100)\nprint(df.shape)\n\ndef seq2ngrams(seqs, n=3):\n    return np.array([[seq[i:i+n] for i in range(len(seq))] for seq in seqs])\n\nmaxlen_seq = 128\ninput_seqs, target_seqs = df[['seq', 'sst3']][(df.len <= maxlen_seq) & (~df.has_nonstd_aa)].values.T\ninput_grams = seq2ngrams(input_seqs)\nprint(len(input_seqs))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\nuse preprocessing tools for text from keras to encode input sequence as word rank numbers  and target sequence as one hot. To ensure easy to use training and testing, all sequences are padded with zeros to the maximum sequence length (in our case 32).","metadata":{"_uuid":"02b0631dff6029c9f5c9eab8bc1afe2b0480ddb4"}},{"cell_type":"code","source":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\ntokenizer_encoder = Tokenizer()\ntokenizer_encoder.fit_on_texts(input_grams)\ninput_data = tokenizer_encoder.texts_to_sequences(input_grams)\ninput_data = sequence.pad_sequences(input_data, maxlen=maxlen_seq, padding='post')\n\ntokenizer_decoder = Tokenizer(char_level=True)\ntokenizer_decoder.fit_on_texts(target_seqs)\ntarget_data = tokenizer_decoder.texts_to_sequences(target_seqs)\ntarget_data = sequence.pad_sequences(target_data, maxlen=maxlen_seq, padding='post')\ntarget_data = to_categorical(target_data)\ninput_data.shape, target_data.shape","metadata":{"_uuid":"0dc3b309d5d2d57c083150b0b7b9e0837cf3cf20","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build model\nThis example is motivated by the NLP-task of POS-tagging. ","metadata":{"_uuid":"c4654982e789d9f811329ba84201517538b035ab"}},{"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n\nn_words = len(tokenizer_encoder.word_index) + 1\nn_tags = len(tokenizer_decoder.word_index) + 1\nprint(n_words, n_tags)\n\ninput = Input(shape=(maxlen_seq,))\nx = Embedding(input_dim=n_words, output_dim=128, input_length=maxlen_seq)(input)\nx = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))(x)\ny = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\nmodel = Model(input, y)\nmodel.summary()","metadata":{"_uuid":"66e75bebfbaea3e539156fc362b0a4cb40095dfe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and evaluate model\nThe model is trained such that the categorical crossentropy is minimized. For evalutation also Q3-accuracy is computed, by computing the accuracy only for coding characters. ","metadata":{"_uuid":"92ed195b0b683254ace122b955e288e843dff81e"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.metrics import categorical_accuracy\nfrom keras import backend  as K\nimport tensorflow as tf\n\ndef q3_acc(y_true, y_pred):\n    y = tf.argmax(y_true, axis=-1)\n    y_ = tf.argmax(y_pred, axis=-1)\n    mask = tf.greater(y, 0)\n    return K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", q3_acc])\n\nX_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=.4, random_state=0)\nseq_train, seq_test, target_train, target_test = train_test_split(input_seqs, target_seqs, test_size=.4, random_state=0)\n\nmodel.fit(X_train, y_train, batch_size=128, epochs=5, validation_data=(X_test, y_test), verbose=1)","metadata":{"_uuid":"10beceaff0e1eb1fba0bf78cd247c58b3c29a1cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\nFor this small example, the model trained very fast and achieves quite reasonable Q3-accuaracies. In the following three training and three testing examples are shown. Each image shows the softmax activation (red) on top of the underlying one hot ground truth (blue).","metadata":{"_uuid":"87da064b3c5af2806a0a72713186d1598fd90056"}},{"cell_type":"code","source":"def onehot_to_seq(oh_seq, index):\n    s = ''\n    for o in oh_seq:\n        i = np.argmax(o)\n        if i != 0:\n            s += index[i]\n        else:\n            break\n    return s\n\ndef plot_results(x, y, y_):\n    print(\"---\")\n    print(\"Input: \" + str(x))\n    print(\"Target: \" + str(onehot_to_seq(y, revsere_decoder_index).upper()))\n    print(\"Result: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n    fig = plt.figure(figsize=(10,2))\n    plt.imshow(y.T, cmap='Blues')\n    plt.imshow(y_.T, cmap='Reds', alpha=.5)\n    plt.yticks(range(4), [' '] + [revsere_decoder_index[i+1].upper() for i in range(3)])\n    plt.show()\n    \nrevsere_decoder_index = {value:key for key,value in tokenizer_decoder.word_index.items()}\nrevsere_encoder_index = {value:key for key,value in tokenizer_encoder.word_index.items()}\n\nN=3\ny_train_pred = model.predict(X_train[:N])\ny_test_pred = model.predict(X_test[:N])\nprint('training')\nfor i in range(N):\n    plot_results(seq_train[i], y_train[i], y_train_pred[i])\nprint('testing')\nfor i in range(N):\n    plot_results(seq_test[i], y_test[i], y_test_pred[i])","metadata":{"_uuid":"8021e5b9b6da12c1133e70dc062a7a85ec0d0ff7","trusted":true},"execution_count":null,"outputs":[]}]}